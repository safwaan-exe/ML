# ğŸ§  Student Stress Prediction â€” Machine Learning Project

## ğŸ“˜ Project Overview
This project analyzes factors affecting **student stress** and builds machine learning classifiers to **predict stress levels**.  
We implemented and compared **Random Forest** and **XGBoost** models, performed **hyperparameter tuning**, and produced a final model recommendation.

---

## ğŸ“‚ Dataset
**Dataset Used:** [Student Stress Factors â€” A Comprehensive Analysis (Kaggle)](https://www.kaggle.com/datasets/rxnach/student-stress-factors-a-comprehensive-analysis)

The dataset includes demographic, academic, and lifestyle attributes of students, along with their stress levels.  
The task was converted into **binary classification**:  
- `0` â†’ Not Stressed  
- `1` â†’ Stressed  

---

## âš™ï¸ Technologies & Libraries
- **Python**
- **Pandas**, **NumPy**
- **Matplotlib**, **Seaborn**
- **Scikit-learn**
- **XGBoost**
- **Google Colab / Jupyter Notebook**

---

## ğŸ§© Project Workflow
1. **Exploratory Data Analysis (EDA)**
   - Inspected dataset shape, datatypes, null values, and distributions.
   - Checked correlations and visualized stress-level relationships.
2. **Data Preprocessing**
   - Dropped irrelevant columns (e.g., `Timestamp`, `User_ID`, etc.).
   - Encoded categorical features using `LabelEncoder`.
   - Standardized numerical features using `StandardScaler`.
   - Converted target to binary form.
3. **Train-Test Split**
   - Used `train_test_split` (80â€“20) with `random_state=42`.
4. **Model Training**
   - Implemented **Random Forest** and **XGBoost** classifiers.
   - Evaluated baseline performance.
5. **Hyperparameter Tuning**
   - Used `RandomizedSearchCV` for both models to find optimal parameters with 5-fold cross-validation.
6. **Evaluation Metrics**
   - Accuracy, Precision, Recall, F1-score, and Confusion Matrix.
7. **Model Comparison**
   - Compared untuned and tuned versions of both models to finalize the best performer.

---

## ğŸ“Š Model Performance Summary

| Model | Train Accuracy | Test Accuracy | CV Accuracy | Remarks |
|-------|----------------|---------------|-------------|----------|
| Random Forest (Untuned) | 100.0% | 90.91% | - | Overfitting observed |
| Random Forest (Tuned) | 100.0% | **92.27%** | 93.41% | Improved generalization |
| XGBoost (Untuned) | 100.0% | 90.91% | - | Strong baseline |
| XGBoost (Tuned) | 95.23% | 91.36% | **93.98%** | Stable & high CV performance |

---

## ğŸ§¾ Evaluation Outputs

### Tuned Random Forest

## ğŸ§¾ Evaluation Outputs

### Tuned Random Forest
ğŸ¯ Train Accuracy: 100.0%
ğŸ¯ Test Accuracy: 92.27%
âœ… Best Parameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 'log2', 'max_depth': 15}
âœ… Best Cross-Validation Accuracy: 93.41%


### Tuned XGBoost
ğŸ¯ Train Accuracy: 95.23%
ğŸ¯ Test Accuracy: 91.36%
âœ… Best Parameters: {'subsample': 0.8, 'reg_lambda': 1, 'reg_alpha': 0.5, 'n_estimators': 100, 'max_depth': 5, 'learning_rate': 0.01, 'gamma': 0.5, 'colsample_bytree': 0.6}
âœ… Best Cross-Validation Accuracy: 93.98%


---

## âœ… Final Conclusion
- **Best Performing Model:** Tuned **Random Forest**  
  - Highest test accuracy: **92.27%**  
  - Slightly better generalization on holdout data.  
- **Alternative Choice:** Tuned **XGBoost**  
  - Excellent cross-validation stability (**93.98%**).  
  - Slightly lower test accuracy but more robust and less overfitting.

---

## ğŸš€ Future Improvements
- Use **SMOTE / ADASYN** for imbalanced data handling.
- Implement **LightGBM** or **CatBoost** for faster training.
- Apply **SHAP / LIME** for model explainability.
- Create a **Streamlit / Flask** web app for interactive stress prediction.
- Deploy the final model with monitoring and versioning (MLOps pipeline).

---

## ğŸ—‚ Project Structure
.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ raw/ # Original dataset
â”‚ â””â”€â”€ processed/ # Cleaned dataset
â”œâ”€â”€ notebooks/
â”‚ â”œâ”€â”€ 01_EDA.ipynb
â”‚ â”œâ”€â”€ 02_Preprocessing.ipynb
â”‚ â”œâ”€â”€ 03_RandomForest.ipynb
â”‚ â””â”€â”€ 04_XGBoost.ipynb
â”œâ”€â”€ models/
â”‚ â”œâ”€â”€ rf_tuned.pkl
â”‚ â””â”€â”€ xgb_tuned.pkl
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ data_preprocess.py
â”‚ â””â”€â”€ model_utils.py
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt


---

## ğŸ‘¨â€ğŸ’» Author
**Safwaan Siddiqui**  
B.Tech CSE (AIML) | Manipal University Jaipur  
GitHub: [@safwaan-exe](https://github.com/safwaan-exe)

---
